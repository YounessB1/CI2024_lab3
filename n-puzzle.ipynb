{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 3\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state , goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    distance = 0\n",
    "    for num in range(1, PUZZLE_DIM**2): \n",
    "        x1, y1 = np.where(state == num)\n",
    "        x2, y2 = np.where(goal == num)\n",
    "        distance += abs(x1 - x2) + abs(y1 - y2)\n",
    "    return int(distance)\n",
    "\n",
    "def difference(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    return np.sum(np.abs(state - goal))\n",
    "\n",
    "\n",
    "def combined(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    return max(manhattan_distance(state, goal), difference(state, goal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(start: np.ndarray, goal: np.ndarray):\n",
    "    # Priority queue (min-heap) initialized with the starting state.\n",
    "    frontier = []\n",
    "    start_tuple = tuple(start.flatten())\n",
    "    heapq.heappush(frontier, (0, start_tuple))\n",
    "    \n",
    "    # Track the best cost to reach each state (number of steps).\n",
    "    cost_so_far = {start_tuple: 0}\n",
    "    \n",
    "    # Set to keep track of visited states to avoid revisits.\n",
    "    visited = set()\n",
    "\n",
    "    # Counter to track the number of times `do_action` is called.\n",
    "    action_count = 0\n",
    "\n",
    "    while frontier:\n",
    "        # Pop the state with the lowest priority (cost + heuristic).\n",
    "        current_priority, current_tuple = heapq.heappop(frontier)\n",
    "        current = np.array(current_tuple).reshape(PUZZLE_DIM, PUZZLE_DIM)\n",
    "        \n",
    "        # Check if we've reached the goal.\n",
    "        if np.array_equal(current, goal):\n",
    "            # Return the total number of actions taken (do_action calls).\n",
    "            return action_count\n",
    "\n",
    "        # Mark the current state as visited.\n",
    "        visited.add(current_tuple)\n",
    "        \n",
    "        # Explore all possible moves from the current state.\n",
    "        for action in available_actions(current):\n",
    "            next_state = do_action(current, action)\n",
    "            \n",
    "            # Increment the action count for each `do_action` call.\n",
    "            action_count += 1\n",
    "            \n",
    "            next_tuple = tuple(next_state.flatten())\n",
    "            \n",
    "            # Calculate the new cost for the next state (increment steps by 1).\n",
    "            new_cost = cost_so_far[current_tuple] + 1\n",
    "            \n",
    "            # Skip if this state has already been visited.\n",
    "            if next_tuple in visited:\n",
    "                continue\n",
    "            \n",
    "            # If it's a new state or found with a lower cost, update it.\n",
    "            if next_tuple not in cost_so_far or new_cost < cost_so_far[next_tuple]:\n",
    "                cost_so_far[next_tuple] = new_cost\n",
    "                # Priority = cost + heuristic (Manhattan distance to goal).\n",
    "                priority = new_cost + manhattan_distance(next_state, goal)\n",
    "                heapq.heappush(frontier, (priority, next_tuple))\n",
    "    \n",
    "    # If the goal is not reachable, return the total number of actions taken.\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = a_star(state, goal_state)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From experiments the best heuristic for this problem is the manhattan disatnce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-7PvHQAZr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
